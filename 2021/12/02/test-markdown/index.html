<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Chapter 5. Machine LearningThe Perceptron Algorithm">
<meta property="og:type" content="article">
<meta property="og:title" content="test_markdown">
<meta property="og:url" content="http://example.com/2021/12/02/test-markdown/index.html">
<meta property="og:site_name" content="Mingwei Yang&#39;s Blog">
<meta property="og:description" content="Chapter 5. Machine LearningThe Perceptron Algorithm">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-12-02T12:03:22.000Z">
<meta property="article:modified_time" content="2021-12-02T12:24:14.397Z">
<meta property="article:author" content="Mingwei Yang">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Test">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2021/12/02/test-markdown/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>test_markdown | Mingwei Yang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Mingwei Yang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/02/test-markdown/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Mingwei Yang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mingwei Yang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          test_markdown
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-12-02 20:03:22 / 修改时间：20:24:14" itemprop="dateCreated datePublished" datetime="2021-12-02T20:03:22+08:00">2021-12-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Algorithm/" itemprop="url" rel="index"><span itemprop="name">Algorithm</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Chapter-5-Machine-Learning"><a href="#Chapter-5-Machine-Learning" class="headerlink" title="Chapter 5. Machine Learning"></a>Chapter 5. Machine Learning</h1><h1 id="The-Perceptron-Algorithm"><a href="#The-Perceptron-Algorithm" class="headerlink" title="The Perceptron Algorithm"></a>The Perceptron Algorithm</h1><span id="more"></span>
<p>The problem of fitting a half-space or a linear separator consists of $n$ labeled examples $x_1,\cdots,x_n$ in $d$-dimensional space. Each example has label $+1$ or $-1$. The task is to find a $d$-dimensional vector $w$, if one exists and a threshold $t$ s.t.</p>
<script type="math/tex; mode=display">
w\cdot x_i>t\text{ for each }x_i\text{ labelled }+1\\
w\cdot x_i<t\text{ for each }x_i\text{ labelled }-1</script><p>A vector-threshold pair, $(w, t)$, satisfying the inequalities is called a “linear separator”.</p>
<p>Add an extra coordinate to each $x_i$ and $w$, writing $\hat{x}_i=(x_i,1)$ and $\hat{w}=(w,-t)$. Suppose $l_i$ is the $\pm 1$ label on $x_i$. Then, the above inequalities can be rewritten as</p>
<script type="math/tex; mode=display">
(\hat{w}\cdot \hat{x}_i)l_i>1</script><h4 id="The-Perceptron-Algorithm-1"><a href="#The-Perceptron-Algorithm-1" class="headerlink" title="The Perceptron Algorithm"></a>The Perceptron Algorithm</h4><p>$w\leftarrow 0$</p>
<p>while there exists $x_i$ with $x_il_i\cdot w\le 0$, update $w\leftarrow w+x_il_i$</p>
<hr>
<p>The intuition is that correcting $w$ by adding $x_il_i$ cause the new $(w\cdot x_i)l_i$ to be higher by $x_i\cdot x_il_i^2=|x_i|^2$. If a weight vector $w^<em>$ satisfies $(w\cdot x_i)l_i&gt;0$ for $1\le i\le n$, the minimum distance of any example $x_i$ to the linear separator $w^</em>\cdot x=0$ is called the <em>margin</em> of the separator. Scale $w^<em>$ so that $(w^</em>\cdot x_i)l_i\ge 1$ for all $i$. Then the margin of the separator is at least $1/|w^<em>|$. If all points lie inside a ball of radius $r$, then $r|w^</em>|$ is the ratio of the radius of the ball to the margin. Theorem below shows that the number of update steps of the Perceptron Algorithm is at most<br>the square of this quantity.</p>
<h4 id="Theorem-5-1"><a href="#Theorem-5-1" class="headerlink" title="Theorem 5.1"></a>Theorem 5.1</h4><p>If there is a $w^<em>$ satisfying $(w^</em>\cdot x_i)l_i\ge 1$ for all $i$, then the Perceptron Algorithm finds a solution $w$ with $(w\cdot x_i)l_i&gt;0$ for all $i$ in at most $r^2|w^*|^2$ updates where $r=\max_i|x_i|$.</p>
<p><strong>Proof</strong> Let $w^<em>$ satisfy the “if” condition of the theorem. Each update increases $w^Tw^</em>$ by at least one.</p>
<script type="math/tex; mode=display">
(w+x_il_i)^Tw^*=w^Tw^*+x_i^Tl_iw^*\ge w^Tw^*+1</script><p>On each update, $|w|^2$ increases by at most $r^2$.</p>
<script type="math/tex; mode=display">
(w+x_il_i)^T(w+x_il_i)=|w|^2+2x_i^Tl_iw+|x_il_i|^2\le |w|^2+|x_i|^2\le |w|^2+r^2</script><p>since $x_i^Tl_iw\le 0$.</p>
<p>If the Perceptron Algorithm makes $m$ updates, then $w^Tw^*\ge m$, and $|w|^2\le mr^2$. Then</p>
<script type="math/tex; mode=display">
m\le |w||w^*|\le\sqrt{m}r|w^*|\implies m\le r^2|w^*|^2</script><p>as desired.</p>
<h1 id="Kernel-Functions-and-Non-linearly-Separable-Data"><a href="#Kernel-Functions-and-Non-linearly-Separable-Data" class="headerlink" title="Kernel Functions and Non-linearly Separable Data"></a>Kernel Functions and Non-linearly Separable Data</h1><p>If the data is not linearly separable, then perhaps one can map the data to a higher dimensional space where it is linearly separable.</p>
<p>If a function $\varphi$ maps the data to another space, one can run the Perceptron Algorithm in the new space. The weight vector will be a linear function $\sum_{i=1}^nc_i\varphi(x_i)$ of the new input data. To determine if a pattern $x_j$ is correctly classified compute</p>
<script type="math/tex; mode=display">
w^T\varphi(x_j)=\sum_{i=1}^nc_i\varphi(x_i)^T\varphi(x_j)</script><p>We do not need to explicitly compute $\varphi$ if we have a function</p>
<script type="math/tex; mode=display">
k(x_i,x_j)=\varphi(x_i)^T\varphi(x_j)</script><p>called a <em>kernel function</em> and <em>kernel matrix</em> $K$ is defined as $k_{ij}=\varphi(x_i)^T\varphi(x_j)$.</p>
<h1 id="Generalizing-to-New-Data"><a href="#Generalizing-to-New-Data" class="headerlink" title="Generalizing to New Data"></a>Generalizing to New Data</h1><h4 id="Formalizing-the-problem"><a href="#Formalizing-the-problem" class="headerlink" title="Formalizing the problem"></a>Formalizing the problem</h4><p>To formalize the learning problem, assume there is some probability distribution $D$ over the instance space $X$, such that</p>
<ol>
<li>our training set $S$ consists of points drawn independently at random from $D$.</li>
<li>our objective is to predict well on new points that are also drawn from $D$.</li>
</ol>
<hr>
<p>Let $c^<em>$, called the </em>target concept<em>, denote the subset of $X$ corresponding to the positive class for a desired binary classification. Our goal is to produce a set $h\subseteq X$, called our </em>hypothesis<em>. The </em>true error<em> of $h$, $err_D(h)$, is the probability it incorrectly classifies a data point drawn at random from $D$. The </em>training error<em> of $h$, $err_S(h)$, is the fraction of points in $S$ on which $h$ and $c^</em>$ disagree.</p>
<p>An hypothesis class $\mathcal{H}$ over $X$ is a collection of subsets of $X$. Given an hypothesis class $\mathcal{H}$ and training set $S$, we aim to find an hypothesis in $\mathcal{H}$ that closely agrees with $c^*$ over $S$.</p>
<h2 id="Overfitting-and-Uniform-Convergence"><a href="#Overfitting-and-Uniform-Convergence" class="headerlink" title="Overfitting and Uniform Convergence"></a>Overfitting and Uniform Convergence</h2><h4 id="Theorem-5-4"><a href="#Theorem-5-4" class="headerlink" title="Theorem 5.4"></a>Theorem 5.4</h4><p>Let $\mathcal{H}$ be an hypothesis class and let $\epsilon$ and $\delta$ be greater than zero. If a training set $S$ of size</p>
<script type="math/tex; mode=display">
n\ge \frac{1}{\epsilon}(\ln|\mathcal{H}|+\ln(1/\delta))</script><p>is drawn from distribution $D$, then with probability greater than or equal to $1-\delta$, every $h\in \mathcal{H}$ with training error zero has true error less than $\epsilon$.</p>
<p><strong>Proof</strong> Let $h_1,h_2,\cdots$ be the hypotheses in $\mathcal{H}$ with true error greater than or equal to $\epsilon$. Consider drawing the sample $S$ of size $n$ and let $A_i$ be the event that $h_i$ has zero training error. Since every $h_i$ has true error greater than or equal to $\epsilon$</p>
<script type="math/tex; mode=display">
\Pr[A_i]\le (1-\epsilon)^n</script><p>By the union bound over all $i$, the probability that any of these $h_i$ is consistent with $S$ is given by</p>
<script type="math/tex; mode=display">
\Pr\left[\bigcup_iA_i\right]\le |\mathcal{H}|(1-\epsilon)^n</script><p>Using the fact that $1-\epsilon\le e^{-\epsilon}$ and replacing $n$ by the sample size bound from the theorem statement, this is at most $\mathcal{H}e^{-\ln|\mathcal{H}|-\ln(1-\delta)}=\delta$ as desired.</p>
<h1 id="VC-Dimension"><a href="#VC-Dimension" class="headerlink" title="VC-Dimension"></a>VC-Dimension</h1><h2 id="Definitions-and-Key-Theorems"><a href="#Definitions-and-Key-Theorems" class="headerlink" title="Definitions and Key Theorems"></a>Definitions and Key Theorems</h2><h4 id="Definition-5-1"><a href="#Definition-5-1" class="headerlink" title="Definition 5.1"></a>Definition 5.1</h4><p>A set system $(X, \mathcal{H})$ consists of a set $X$ and a class $\mathcal{H}$ of subsets of $X$.</p>
<hr>
<p>$\mathcal{H}$ is the class of potential hypothesis, where a hypothesis $h$ is a subset of $X$.</p>
<h4 id="Definition-5-2"><a href="#Definition-5-2" class="headerlink" title="Definition 5.2"></a>Definition 5.2</h4><p>A set system $(X, \mathcal{H})$ shatters a set $A$ if each subset of $A$ can be expressed as $A\cap H$ for some $h$ in $\mathcal{H}$.</p>
<h4 id="Definition-5-3"><a href="#Definition-5-3" class="headerlink" title="Definition 5.3"></a>Definition 5.3</h4><p>The <strong>VC-dimension</strong> of $\mathcal{H}$ is the size of the largest set shattered by $\mathcal{H}$.</p>
<h2 id="VD-Dimension-of-Some-Set-System"><a href="#VD-Dimension-of-Some-Set-System" class="headerlink" title="VD-Dimension of Some Set System"></a>VD-Dimension of Some Set System</h2><h4 id="Intervals-of-the-reals"><a href="#Intervals-of-the-reals" class="headerlink" title="Intervals of the reals"></a>Intervals of the reals</h4><p>Intervals on the real line can shatter any set of two points but no set of three points since the subset of the first and last points cannot be isolated. Thus, the VC-dimension of intervals is two.</p>
<h4 id="Pairs-of-intervals-of-the-reals"><a href="#Pairs-of-intervals-of-the-reals" class="headerlink" title="Pairs of intervals of the reals"></a>Pairs of intervals of the reals</h4><p>Consider the family of pairs of intervals, where a pair of intervals is viewed as the set of points that are in at least one of the intervals. There exists a set of size four that can be shattered but no set of size five since the subset of first, third, and last point cannot be isolated. Thus, the VC-dimension of pairs of intervals is four.</p>
<h4 id="Convex-polygons"><a href="#Convex-polygons" class="headerlink" title="Convex polygons"></a>Convex polygons</h4><p>For any positive integer $n$, place $n$ points on the unit circle. Any subset of the points are the vertices of a convex polygon. Clearly that polygon does not contain any of the points not in the subset. This shows that convex polygons can shatter arbitrarily large sets, so the VC-dimension is infinite.</p>
<h4 id="Halfspace-in-d-dimensions"><a href="#Halfspace-in-d-dimensions" class="headerlink" title="Halfspace in $d$-dimensions"></a>Halfspace in $d$-dimensions</h4><p>The VC-dimension of halfspaces in $d$-dimensions is $d+1$.</p>
<p>There exists a set of size $d + 1$ that can be shattered by halfspaces. Select the $d$ unit coordinate vectors plus the origin to be the $d+1$ points. Suppose $A$ is any subset of these $d+1$ points. Without loss of generality assume that the origin is in $A$. Take a 0-1 vector $w$ which has $1$’s precisely in the coordinates corresponding to vectors not in $A$. Clearly $A$ lies in the half-space $w^Tx\le 0$ and the complement of $A$ lies in the complementary halfspace.</p>
<p>We now show that no set of $d + 2$ points in $d$-dimensions can be shattered by halfspaces. This is done by proving that any set of $d + 2$ points can be partitioned into two disjoint subsets $A$ and $B$ whose convex hulls intersect. This establishes the claim since any linear separator with $A$ on one side must have its entire convex hull on that side, so it is not possible to have a linear separator with $A$ on one side and $B$ on the other.</p>
<h4 id="Theorem-5-9-Radon"><a href="#Theorem-5-9-Radon" class="headerlink" title="Theorem 5.9 (Radon)"></a>Theorem 5.9 (Radon)</h4><p>Any set $S\subseteq R^d$ with $|S|\ge d+2$, can be partitioned into two disjoint subsets $A$ and $B$ such that $convex(A)\cap convex(B)\neq \empty$.</p>
<p><strong>Proof</strong> Assume $|S|=d+2$. Form a $d\times (d+2)$ matrix $A$ with one column for each point of $S$. Add an extra row of all 1’s to construct a $(d+1)\times (d+2)$ matrix $B$. Say $x=(x_1,\cdots,x_{d+2})$ is a non-zero vector with $Bx=0$. Reorder the columns so that $x_1,\cdots,x_s\ge 0$ and $x_{s+1},\cdots,x_{d+2}&lt;0$. Normalize $x$ so $\sum_{i=1}^s|x_i|=1$. Let $a_i$ be the $i^{th}$ column of $A$. Then, $\sum_{i=1}^s|x_i|a_i=\sum_{i=s+1}^{d+2}|x_i|a_i$ and $\sum_{i=1}^s|x_i|=\sum_{i=s+1}^{d+2}|x_i|$. Since $\sum_{i=1}^s|x_i|=\sum_{i=s+1}^{d+2}|x_i|=1$, each side of $\sum_{i=1}^s|x_i|a_i=\sum_{i=s+1}^{d+2}|x_i|a_i$ is a convex combination of columns of $A$, which proves the theorem.</p>
<h2 id="Shatter-Function-for-Set-Systems-of-Bounded-VC-Dimension"><a href="#Shatter-Function-for-Set-Systems-of-Bounded-VC-Dimension" class="headerlink" title="Shatter Function for Set Systems of Bounded VC-Dimension"></a>Shatter Function for Set Systems of Bounded VC-Dimension</h2><p>For a set system $(X,\mathcal{H})$, the shatter function $\pi_{\mathcal{H}}(n)$ is the maximum number of subsets of any set $A$ of size $n$ that can be expressed as $A\cap h$ for $h$ in $\mathcal{H}$. The function $\pi_{\mathcal{H}}(n)$ equals $2^n$ for $n$ less than or equal to the VC-dimension of $\mathcal{H}$. Define</p>
<script type="math/tex; mode=display">
\binom{n}{\le d}=\binom{n}{0}+\cdots+\binom{n}{d}\le n^d+1</script><p>The inequality holds because to choose between $1$ and $d$ elements out of $n$, for each position there are $n$ possible items if we allow duplicates. The $1$ is for $\binom{n}{0}$.</p>
<h4 id="Lemma-5-10-Sauer"><a href="#Lemma-5-10-Sauer" class="headerlink" title="Lemma 5.10 (Sauer)"></a>Lemma 5.10 (Sauer)</h4><p>For any set system $(X,\mathcal{H})$ of VC-dimension at most $d$, $\pi_{\mathcal{H}}(n)\le \binom{n}{\le d}$ for all $n$.</p>
<h2 id="VC-Dimension-of-Combinations-of-Concepts"><a href="#VC-Dimension-of-Combinations-of-Concepts" class="headerlink" title="VC-Dimension of Combinations of Concepts"></a>VC-Dimension of Combinations of Concepts</h2><p>Let $(X,\mathcal{H}_1)$ and $(X,\mathcal{H}_2)$ be two set systems. Define intersection system $(X,\mathcal{H}_1\cap \mathcal{H}_2)$, where $\mathcal{H}_1\cap \mathcal{H}_2=\{h_1\cap h_2|h_1\in \mathcal{H}_1,h_2\in\mathcal{H}_2\}$.</p>
<h4 id="Lemma-5-11"><a href="#Lemma-5-11" class="headerlink" title="Lemma 5.11"></a>Lemma 5.11</h4><p>Suppose $(X,\mathcal{H}_1)$ and $(X,\mathcal{H}_2)$ are two set systems on the same set $X$. Then</p>
<script type="math/tex; mode=display">
\pi_{\mathcal{H}_1\cap \mathcal{H}_2}(n)\le \pi_{\mathcal{H}_1}(n)\pi_{\mathcal{H}_2}(n)</script><p><strong>Proof</strong> Let $A\subset X$ and $\mathcal{S}=\{A\cap h|h\in \mathcal{H}_1\cap\mathcal{H}_2\}$. Let $h=h_1\cap h_2$. Then $A\cap h=(A\cap h_1)\cap (A\cap h_2)$. Therefore, $|S|\le |\{A\cap h_1|h_1\in \mathcal{H}_1\}|\cdot |\{A\cap h_2|h_2\in \mathcal{H}_2\}|$, as desired.</p>
<h2 id="The-Key-Theorem"><a href="#The-Key-Theorem" class="headerlink" title="The Key Theorem"></a>The Key Theorem</h2><h4 id="Theorem-5-14"><a href="#Theorem-5-14" class="headerlink" title="Theorem 5.14"></a>Theorem 5.14</h4><p>Let $(X,\mathcal{H})$ be a set system, $D$ a probability distribution over $X$, and let $n$ be an integer satisfying</p>
<script type="math/tex; mode=display">
n\ge \frac{2}{\epsilon}\left[\log_22\pi_{\mathcal{H}}(2n)+\log_2\frac{1}{\delta}\right]</script><p>Let $S_1$ consists of $n$ points drawn from $D$. With probability greater than or equal to $1-\delta$, every set in $\mathcal{H}$ of probability mass greater than $\epsilon$ intersects $S_1$.</p>
<p><strong>Proof</strong> Let $A$ be the event that there exists a set $h$ in $\mathcal{H}$ of probability mass greater than or equal to $\epsilon$ that is disjoint from $S_1$. Draw a second set $S_2$ of $n$ points from $D$. Let $B$ be the event that there exists $h$ in $\mathcal{H}$ that is disjoint from $S_1$ but that contains at least $\frac{\epsilon}{2}n$ points in $S_2$. </p>
<p>By Chebyshev, $\Pr[B|A]\ge\frac{1}{2}$. This means that</p>
<script type="math/tex; mode=display">
\Pr[B]\ge\Pr[A,B]=\Pr[B|A]\Pr[A]\ge\frac{1}{2}\Pr[A]</script><p>Therefore, it suffices to prove that $\Pr[B]\le\frac{\delta}{2}$. Consider a second way of picking $S_1$ and $S_2$. Draw a random set $S_3$ of $2n$ points from $D$, and then randomly partition $S_3$ into two equal pieces $S_1$ and $S_2$. </p>
<p>Consider the point in time after $S_3$ has been drawn but before it has been randomly partitioned. $\mathcal{H}$ has at most $\pi_{\mathcal{H}}(2n)$ distinct intersections with $S_3$. To prove that $\Pr[B]\le\frac{\delta}{2}$, it is sufficient to prove that for any $h’\subseteq S_3$, the probability that $|S_1\cap h’|=0$ but $|S_2\cap h’|\ge\frac{\epsilon}{2}n$ is at most $\frac{\delta}{2\pi_{\mathcal{H}}(2n)}$.</p>
<p>Note that if $h’$ contains fewer than $\frac{\epsilon}{2}n$ points, it is impossible to have $S_2\cap h’\ge\frac{\epsilon}{2}n$. For $h’$ larger than $\frac{\epsilon}{2}n$, the probability that none of the points in $h’$ fall into $S_1$ is at most $(\frac{1}{2})^{\epsilon n/2}$ (negative correlation). Plugging in our bound on $n$ get the desired result.</p>
<h1 id="VC-dimension-and-Machine-Learning"><a href="#VC-dimension-and-Machine-Learning" class="headerlink" title="VC-dimension and Machine Learning"></a>VC-dimension and Machine Learning</h1><p>We have a target concept $c^<em>$ and a set of hypotheses $\mathcal{H}$. Let $\mathcal{H}’=\{h\Delta c^</em>|h\in \mathcal{H}\}$ be the collection of error regions of hypotheses in $\mathcal{H}$, where $\Delta$ refers to symmetry difference. Note that $\mathcal{H}’$ and $\mathcal{H}$ have the same VC-dimension and shatter function.</p>
<h4 id="Theorem-5-15-sample-bound"><a href="#Theorem-5-15-sample-bound" class="headerlink" title="Theorem 5.15 (sample bound)"></a>Theorem 5.15 (sample bound)</h4><p>For any class $\mathcal{H}$ and distribution $D$, if a training sample $S$ is drawn from $D$ of size</p>
<script type="math/tex; mode=display">
n\ge\frac{2}{\epsilon}\left[\log(2\pi_{\mathcal{H}}(2n))+\log\frac{1}{\delta}\right]</script><p>then with probability greater than or equal to $1-\delta$, every $h\in\mathcal{H}$ with true error $err_D(h)\ge\epsilon$ has $err_S(h)&gt;0$.</p>
<p><strong>Proof</strong> The proof follows from Theorem 5.14 applied to $\mathcal{H}’$.</p>
<h1 id="Online-Learning"><a href="#Online-Learning" class="headerlink" title="Online Learning"></a>Online Learning</h1>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Test/" rel="tag"># Test</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/1970/01/01/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Chapter-5-Machine-Learning"><span class="nav-number">1.</span> <span class="nav-text">Chapter 5. Machine Learning</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Perceptron-Algorithm"><span class="nav-number">2.</span> <span class="nav-text">The Perceptron Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-Perceptron-Algorithm-1"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">The Perceptron Algorithm</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Theorem-5-1"><span class="nav-number">2.0.0.2.</span> <span class="nav-text">Theorem 5.1</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kernel-Functions-and-Non-linearly-Separable-Data"><span class="nav-number">3.</span> <span class="nav-text">Kernel Functions and Non-linearly Separable Data</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Generalizing-to-New-Data"><span class="nav-number">4.</span> <span class="nav-text">Generalizing to New Data</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Formalizing-the-problem"><span class="nav-number">4.0.0.1.</span> <span class="nav-text">Formalizing the problem</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Overfitting-and-Uniform-Convergence"><span class="nav-number">4.1.</span> <span class="nav-text">Overfitting and Uniform Convergence</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Theorem-5-4"><span class="nav-number">4.1.0.1.</span> <span class="nav-text">Theorem 5.4</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VC-Dimension"><span class="nav-number">5.</span> <span class="nav-text">VC-Dimension</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Definitions-and-Key-Theorems"><span class="nav-number">5.1.</span> <span class="nav-text">Definitions and Key Theorems</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Definition-5-1"><span class="nav-number">5.1.0.1.</span> <span class="nav-text">Definition 5.1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Definition-5-2"><span class="nav-number">5.1.0.2.</span> <span class="nav-text">Definition 5.2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Definition-5-3"><span class="nav-number">5.1.0.3.</span> <span class="nav-text">Definition 5.3</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VD-Dimension-of-Some-Set-System"><span class="nav-number">5.2.</span> <span class="nav-text">VD-Dimension of Some Set System</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Intervals-of-the-reals"><span class="nav-number">5.2.0.1.</span> <span class="nav-text">Intervals of the reals</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pairs-of-intervals-of-the-reals"><span class="nav-number">5.2.0.2.</span> <span class="nav-text">Pairs of intervals of the reals</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Convex-polygons"><span class="nav-number">5.2.0.3.</span> <span class="nav-text">Convex polygons</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Halfspace-in-d-dimensions"><span class="nav-number">5.2.0.4.</span> <span class="nav-text">Halfspace in $d$-dimensions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Theorem-5-9-Radon"><span class="nav-number">5.2.0.5.</span> <span class="nav-text">Theorem 5.9 (Radon)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Shatter-Function-for-Set-Systems-of-Bounded-VC-Dimension"><span class="nav-number">5.3.</span> <span class="nav-text">Shatter Function for Set Systems of Bounded VC-Dimension</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Lemma-5-10-Sauer"><span class="nav-number">5.3.0.1.</span> <span class="nav-text">Lemma 5.10 (Sauer)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VC-Dimension-of-Combinations-of-Concepts"><span class="nav-number">5.4.</span> <span class="nav-text">VC-Dimension of Combinations of Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Lemma-5-11"><span class="nav-number">5.4.0.1.</span> <span class="nav-text">Lemma 5.11</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Key-Theorem"><span class="nav-number">5.5.</span> <span class="nav-text">The Key Theorem</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Theorem-5-14"><span class="nav-number">5.5.0.1.</span> <span class="nav-text">Theorem 5.14</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VC-dimension-and-Machine-Learning"><span class="nav-number">6.</span> <span class="nav-text">VC-dimension and Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Theorem-5-15-sample-bound"><span class="nav-number">6.0.0.1.</span> <span class="nav-text">Theorem 5.15 (sample bound)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Online-Learning"><span class="nav-number">7.</span> <span class="nav-text">Online Learning</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Mingwei Yang"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Mingwei Yang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mingwei Yang</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
